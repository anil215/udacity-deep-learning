{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 28, 28) (50000,)\n",
      "Validation set (1000, 28, 28) (1000,)\n",
      "Test set (1000, 28, 28) (1000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 784) (50000, 10)\n",
      "Validation set (1000, 784) (1000, 10)\n",
      "Test set (1000, 784) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.940277\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.9%\n",
      "Minibatch loss at step 500: 2.865490\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 1000: 1.713219\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 1500: 1.213735\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 2000: 0.979322\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2500: 0.705286\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 3000: 0.602646\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.6%\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value of this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2fd7aacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization(logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1 layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "      tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "      \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 724.376160\n",
      "Minibatch accuracy: 7%\n",
      "Validation accuracy: 31%\n",
      "Minibatch loss at step 500: 196.505783\n",
      "Minibatch accuracy: 83%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 1000: 116.186966\n",
      "Minibatch accuracy: 84%\n",
      "Validation accuracy: 80%\n",
      "Minibatch loss at step 1500: 69.093033\n",
      "Minibatch accuracy: 86%\n",
      "Validation accuracy: 82%\n",
      "Minibatch loss at step 2000: 41.543285\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 83%\n",
      "Minibatch loss at step 2500: 25.305973\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 86%\n",
      "Minibatch loss at step 3000: 15.477118\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 86%\n",
      "Test accuracy: 93%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data , which has been randomized.\n",
    "        # Note: we could use better randomization across epochs\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500)==0:\n",
    "            print('Minibatch loss at step %d: %f' % (step,l))\n",
    "            print('Minibatch accuracy: %.lf%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.lf%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.lf%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEOCAYAAACD5gx6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcsmKRxKvtX+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2d769dc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization(logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "      tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) \n",
    "      \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 410.889191\n",
      "Minibatch accuracy: 9%\n",
      "Validation accuracy: 34%\n",
      "Minibatch loss at step 2: 1522.432495\n",
      "Minibatch accuracy: 29%\n",
      "Validation accuracy: 47%\n",
      "Minibatch loss at step 4: 256.839478\n",
      "Minibatch accuracy: 59%\n",
      "Validation accuracy: 57%\n",
      "Minibatch loss at step 6: 170.990204\n",
      "Minibatch accuracy: 71%\n",
      "Validation accuracy: 69%\n",
      "Minibatch loss at step 8: 46.595348\n",
      "Minibatch accuracy: 84%\n",
      "Validation accuracy: 71%\n",
      "Minibatch loss at step 10: 48.809170\n",
      "Minibatch accuracy: 88%\n",
      "Validation accuracy: 74%\n",
      "Minibatch loss at step 12: 7.964755\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 14: 2.635299\n",
      "Minibatch accuracy: 96%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 16: 10.810403\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 18: 1.104649\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 20: 5.390417\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 22: 10.509593\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 26: 4.680853\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 28: 10.359469\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 32: 4.569305\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 34: 10.112584\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 38: 4.460416\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 40: 9.871579\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 44: 4.354131\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 46: 9.636254\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 48: 0.000007\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 50: 4.250834\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 52: 9.403982\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 54: 0.000049\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 56: 4.154659\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 58: 9.175183\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 60: 0.000049\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 62: 4.060683\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 64: 8.951914\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 66: 0.000050\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 68: 3.968946\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 70: 8.733957\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 72: 0.000050\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 74: 3.879414\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 76: 8.521053\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 78: 0.000051\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 80: 3.800603\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 82: 8.305367\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 84: 0.000052\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 86: 3.759054\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 88: 8.064572\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 90: 0.000056\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 92: 3.677583\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 94: 7.864779\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Minibatch loss at step 96: 0.000063\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 98: 3.598526\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 77%\n",
      "Minibatch loss at step 100: 7.669884\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 75%\n",
      "Test accuracy: 81%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data , which has been randomized.\n",
    "        # Note: we could use better randomization across epochs\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 2)==0:\n",
    "            print('Minibatch loss at step %d: %f' % (step,l))\n",
    "            print('Minibatch accuracy: %.lf%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.lf%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.lf%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is close to 100%. The generalization capability is poor, as shown in the validation and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits,labels = tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 538.164001\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 31.9%\n",
      "Minibatch loss at step 2: 995.832336\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 36.2%\n",
      "Minibatch loss at step 4: 264.656494\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 57.6%\n",
      "Minibatch loss at step 6: 10.974018\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 8: 1.169773\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 10: 1.890962\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 12: 1.209068\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 14: 1.380029\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 16: 0.225778\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 18: 0.080969\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 22: 0.087798\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 32: 0.247098\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 34: 0.571723\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 40: 0.178984\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 42: 0.326837\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 50: 0.558024\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 52: 0.019741\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 56: 1.033121\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-869792076c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     _, l, predictions = session.run(\n\u001b[0;32m---> 20\u001b[0;31m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetCode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m       raise _make_specific_exception(\n\u001b[1;32m    464\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels= tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.266801\n",
      "Minibatch accuracy: 5%\n",
      "Validation accuracy: 38%\n",
      "Minibatch loss at step 500: 0.877797\n",
      "Minibatch accuracy: 90%\n",
      "Validation accuracy: 84%\n",
      "Minibatch loss at step 1000: 0.773705\n",
      "Minibatch accuracy: 87%\n",
      "Validation accuracy: 87%\n",
      "Minibatch loss at step 1500: 0.673934\n",
      "Minibatch accuracy: 88%\n",
      "Validation accuracy: 87%\n",
      "Minibatch loss at step 2000: 0.583617\n",
      "Minibatch accuracy: 88%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 2500: 0.491996\n",
      "Minibatch accuracy: 93%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 3000: 0.386987\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 3500: 0.388051\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 4000: 0.419525\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 4500: 0.414285\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 5000: 0.347748\n",
      "Minibatch accuracy: 96%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 5500: 0.327833\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 6000: 0.368826\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 6500: 0.338484\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 7000: 0.321086\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 7500: 0.324315\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 8000: 0.346857\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 8500: 0.271263\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 9000: 0.369876\n",
      "Minibatch accuracy: 96%\n",
      "Validation accuracy: 89%\n",
      "Test accuracy: 95%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick d an offset within the training data which has been randomized\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # prepare a dictionary telling teh session where to feed the minibatch\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "           [optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500) == 0:\n",
    "            print('Minibatch loss at step %d: %f' %(step, l))\n",
    "            print('Minibatch accuracy: %.lf%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.lf%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.lf%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try one layer deeper with dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels= tf_train_labels)) \n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.265962\n",
      "Minibatch accuracy: 14%\n",
      "Validation accuracy: 38%\n",
      "Minibatch loss at step 500: 0.260913\n",
      "Minibatch accuracy: 90%\n",
      "Validation accuracy: 86%\n",
      "Minibatch loss at step 1000: 0.301121\n",
      "Minibatch accuracy: 88%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 1500: 0.331055\n",
      "Minibatch accuracy: 89%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 2000: 0.171574\n",
      "Minibatch accuracy: 94%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 2500: 0.094455\n",
      "Minibatch accuracy: 97%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 3000: 0.058087\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 3500: 0.064539\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 4000: 0.045151\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 4500: 0.043578\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 5000: 0.012809\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 5500: 0.013281\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 6000: 0.041664\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 6500: 0.037940\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 7000: 0.016613\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 7500: 0.010244\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 8000: 0.017174\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 8500: 0.004744\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 9000: 0.015806\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 9500: 0.020675\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 10000: 0.012961\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 10500: 0.003290\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 11000: 0.003581\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 11500: 0.027464\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 12000: 0.013867\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 12500: 0.008033\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 13000: 0.005091\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 13500: 0.010442\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 14000: 0.007560\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 14500: 0.003264\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 15000: 0.113221\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 15500: 0.006264\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 16000: 0.037821\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 16500: 0.006927\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 17000: 0.002881\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 17500: 0.016543\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 18000: 0.028967\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Test accuracy: 96%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick d an offset within the training data which has been randomized\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # prepare a dictionary telling teh session where to feed the minibatch\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "           [optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500) == 0:\n",
    "            print('Minibatch loss at step %d: %f' %(step, l))\n",
    "            print('Minibatch accuracy: %.lf%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.lf%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.lf%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels= tf_train_labels)) \n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.366558\n",
      "Minibatch accuracy: 9%\n",
      "Validation accuracy: 34%\n",
      "Minibatch loss at step 500: 0.264025\n",
      "Minibatch accuracy: 91%\n",
      "Validation accuracy: 86%\n",
      "Minibatch loss at step 1000: 0.289910\n",
      "Minibatch accuracy: 91%\n",
      "Validation accuracy: 88%\n",
      "Minibatch loss at step 1500: 0.242092\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 87%\n",
      "Minibatch loss at step 2000: 0.149117\n",
      "Minibatch accuracy: 95%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 2500: 0.041286\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 3000: 0.046935\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 3500: 0.017020\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 4000: 0.044348\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 4500: 0.014788\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 5000: 0.009961\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 5500: 0.004404\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 6000: 0.007498\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 6500: 0.026697\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 7000: 0.014369\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 89%\n",
      "Minibatch loss at step 7500: 0.008042\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 8000: 0.010367\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 8500: 0.003677\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 9000: 0.006521\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 9500: 0.010158\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 10000: 0.005482\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 10500: 0.002435\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 11000: 0.002142\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 11500: 0.020082\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 12000: 0.005732\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 12500: 0.003978\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 13000: 0.003337\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 13500: 0.005810\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 14000: 0.005855\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 14500: 0.001587\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 15000: 0.090727\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 15500: 0.003288\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 16000: 0.037274\n",
      "Minibatch accuracy: 98%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 16500: 0.006137\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 17000: 0.001115\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 17500: 0.012337\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 18000: 0.023543\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 18500: 0.025978\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 19000: 0.039366\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 19500: 0.022794\n",
      "Minibatch accuracy: 99%\n",
      "Validation accuracy: 90%\n",
      "Minibatch loss at step 20000: 0.006538\n",
      "Minibatch accuracy: 100%\n",
      "Validation accuracy: 90%\n",
      "Test accuracy: 96%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick d an offset within the training data which has been randomized\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # prepare a dictionary telling teh session where to feed the minibatch\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "           [optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500) == 0:\n",
    "            print('Minibatch loss at step %d: %f' %(step, l))\n",
    "            print('Minibatch accuracy: %.lf%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.lf%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.lf%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
