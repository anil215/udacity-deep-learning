{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "# helper functions to convert each lower case character to an id and vice versa\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch() # a 2d numpy array with one hot encoded vectors\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float) # keeping one hot encoded vectors for each letter\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0 # just setting the required position to 1 for one hot encoding\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296231 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "vammoztdve heeyg dexserotkijo  qhkep nmct r wbixoibyfvjpiq aonhilmih idsv ls  nn\n",
      "egzog  tuabcopesadhhexu rrwiiisuei ifeeswarawsupw m dpi bjeqppkpztysrsttintlbxnh\n",
      "ju  rubsfpv tov pcknotacwkld x xik knoenakpmorhovpjciauoc imcpsgotlt b afhexqhji\n",
      "dgeeifadajtd isgfiiq jdrdfg pixkddla k bsd c d qg deopqst xte n  j xoaod xgk onh\n",
      "zifwesjs hexpfxistortjm qcznttmf jaizioxn hguoyvtedeurvrmywiiosn erhexri ns  aus\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.596055 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 10.35\n",
      "Average loss at step 200: 2.254662 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.55\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 300: 2.098719 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.001130 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 500: 1.937587 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 600: 1.909239 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 700: 1.858562 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.817585 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.828613 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.824989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "ga in recate two ecoms actembnal have sequily mouta one eight seven five seven n\n",
      "ri ow for thot exam transe the under setern of the vederey and buchenined sre su\n",
      "heris neture jo khow word one nive four on the sy for evely seven smour une us o\n",
      "ter will desen exprise benin the base one three seven jime prester spy he treadi\n",
      "gen seconted jaugh of the desided the achausing chiture erambuling of emectting \n",
      "================================================================================\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1100: 1.775812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.752614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1300: 1.734722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1400: 1.747222 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.738171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.746371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1700: 1.709426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.675522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.646562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.696100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "or lijearnor ary reducty as many before the proper jost elanding socents commeni\n",
      "mate was sbitters in one nine five six four a one nine five give zero four excha\n",
      "x tasty is empersing smatides woren matted is indiair in to ppect in one eight s\n",
      "t mis two zero zero five five eight einenear todain parniculbuides at that is ir\n",
      " suths a fliancessianner in expling in the fine destriles yects on lobingdona in\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.682416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2200: 1.676918 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2300: 1.641574 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2400: 1.657175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2500: 1.676069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2600: 1.649350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.655727 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.646732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.648042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3000: 1.646323 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "ative press he andremptionity cantically of the may been thingersomet foritical \n",
      "ver and dectricts in calles the to pappitive one nine seven nine seven zero two \n",
      "heral reffeently seven theologe ppits ballzer with in all ging and to recoirt an\n",
      "x payedok was there of appetion reampent resoincl evication function homanchm no\n",
      "stem two yearligle buil and to communia of taper wasy payaring the pnysected sus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3100: 1.622995 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200: 1.642426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3300: 1.634669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.665696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3500: 1.652748 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3600: 1.661862 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.645930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3800: 1.640369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.632639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.651106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "amity popt rotes obser an imperrend prophed derepeniams have rud conftrative cer\n",
      "ently otherms emphory list provints from his once of italhmic genealogican or he\n",
      "y hfasce gho has formate inycape this freed to which cotere promics proptagen e \n",
      "gual and photakes deslantting the force the perschking komon spied lepar agautis\n",
      "d mests amoned the during in the monntarir treupied puesa wiver kid citete since\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4100: 1.626085 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.631754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.612533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4400: 1.603394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.609204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.608916 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.614308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4800: 1.623781 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4900: 1.629123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5000: 1.597479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "bles ser the new coine sudden one nine zero zero zero a yeashed fogors confrom d\n",
      "s by bory nimp day bwidon of kbatlate aarame times they only huimint shorted wor\n",
      "lary falcy not positive poden one nine five two creates the fukhers the the forc\n",
      "grad for more roze jonesuze fasco one six one zero zero zero zero zero three bri\n",
      "sup ball often congantigned to ener destrian to pennime devance of a hadd or hig\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5100: 1.597996 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5200: 1.586265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.575810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.576649 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.564440 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.577425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5700: 1.564077 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.576434 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5900: 1.571285 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6000: 1.544947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ine shary nortaelaue eight advial ab be the has the forendactifice wingringids a\n",
      "zer boeth world and neves gruns race as the vieweng of urish hirous the comprops\n",
      "dot here american on whom which is exchuse is deat gase into be curistary sallel\n",
      "nea quiciday a sweding and taller many naburies this riverse from the formers of\n",
      " deforms a is estended classity porton resexbed incombert crismance is is forces\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.561946 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6200: 1.530650 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6300: 1.538573 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6400: 1.532136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.554466 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6600: 1.590395 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6700: 1.572142 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.598670 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6900: 1.578818 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000: 1.570865 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "mers two zero six four lawnerian burign presecatide to the center he facilly b t\n",
      "x in alborgadion collected links of new formoar one sombtioned at which of the p\n",
      "ney between of all high crieded a to of the listly standate have of nanwards to \n",
      "braic lawn of english in ampletist soyd been bean name of gold s writtsh to be i\n",
      "ot the danishee an one nine nine six one five names s dowed by lebsour aid the z\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input Gate: input, previous output, bias\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size,num_nodes],-0.1,0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    ib = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    # Forget gate: input , previous output, and bias\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size,num_nodes],-0.1,0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes,num_nodes], -0.1,0.1))\n",
    "    fb = tf.Variable(tf.truncated_normal([1,num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox],1)\n",
    "    m = tf.concat([im, fm, cm, om],1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob],1)\n",
    "    # Definition of the cell computation\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294121 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "hafpw  ect bmyilc nycsdrsakrsso ygsw hio exd dsiriqkp smvkmrd rn mom  se ehenzrw\n",
      "xe tlqy zxftwgve k m skfecjyqellh ate pbiwfoenzej mgvrpsqcac  ff  giantbrno uhzt\n",
      "fw kwynausar tt gupryn d uolir lqdaiapou be  fegza heot tcszgxvii   n ysot  yr  \n",
      "vntrejpvpsh ms noyxgtcax wx dtyjlndutvgyyxtlegcmsetemwckxwdjspatp ka tjloidq lec\n",
      "n itsyniootidel   saaasgfd  flirdixmsioryeymselozgeqeqitatjl  ok hiijvleecxeefql\n",
      "================================================================================\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 100: 2.595639 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.02\n",
      "Validation set perplexity: 11.03\n",
      "Average loss at step 200: 2.249916 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.39\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 300: 2.092034 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.035438 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 500: 1.981497 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 1.900500 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.874176 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 800: 1.867341 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 900: 1.847815 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1000: 1.845211 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "================================================================================\n",
      "zer of firsi s the carain firjents terbees the lise uses the seftecters of gulag\n",
      "lly surchoingh dioneve had at the wits ref eptant detwosed dlen genau deament of\n",
      "ts as hoory welbparisher landen devarfs codeds by mine s earped mamme wides wedp\n",
      "zers dune spored eight a peasly one eight a mass k coure the marious gate then a\n",
      "j seosionys two paunch act s one nine eight one four zero one eight eight a seth\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.801527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1200: 1.776359 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1300: 1.763161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1400: 1.763667 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1500: 1.752570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1600: 1.734559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.718834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1800: 1.691752 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1900: 1.699000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.681636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "ers kides muel gases and utternan overfurchy on an offices by monstamfutes conni\n",
      "mencrih and destniffed ticks dontwe to be iment depeniationsmed vist constantal \n",
      "an ina tewalogy servely misics of clesticult contented apperiet of the liwlexs m\n",
      "zer with soven alobank traoted from height natured ty of its as regiles gien beh\n",
      "ers condcter of dobel thank caelfor filmcofical off inotia hitfid beforma is wer\n",
      "================================================================================\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2100: 1.687615 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2200: 1.705610 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2300: 1.710521 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.686933 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.688999 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2600: 1.671858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2700: 1.685304 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2800: 1.679169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2900: 1.676789 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.685760 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "rankents chandal frense cyt the part of the first to changate ta symburate the c\n",
      "bon os the lang watfresent of class as the bexore other eccress over a sepabilan\n",
      "rius flow ded to and nours of stokod of a senthels of the three sove formation o\n",
      "zed and fresion abard the action work may to aproclogoders and dare nutss a pann\n",
      "quie fly it is op that wobld form field or the pown fores see vean the buteectia\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3100: 1.652517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3200: 1.637022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3300: 1.642208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3400: 1.635498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3500: 1.677100 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3600: 1.651396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3700: 1.653509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3800: 1.655083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.652619 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4000: 1.644004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "x porls gooms hubyer airers subference and first eight four are fizais id sham r\n",
      "p coall omfathing profirds subfe of paciet was attess fear pook trare and the al\n",
      " s meaming missolus propycander ba hes information and in examptian thearrkes co\n",
      "mp ned kower backs on semaia overy image of plannell of her quarner the goup a f\n",
      "historius becain acripaters frampentiage of eductay a from poom cwreees and ftus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4100: 1.620136 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.618097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4300: 1.619820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4400: 1.606639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.639090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4600: 1.623393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4700: 1.625333 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.605613 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4900: 1.620523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5000: 1.620409 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "rance techriercee tor eight the also lobe wling was blanu atsont on now an doec \n",
      "rear cutizets that sour eich five it were englys a currently winesphin a of ruge\n",
      "joleter trirdide of the truplant be boll of the four fail seven p occofts the fe\n",
      "man be now tune winnee on this five one eight seven nine fretmer on a roiv libel\n",
      "s to recael the korrotor diatepe on shor presented of the gene also bean arecy d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5100: 1.592157 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5200: 1.598683 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5300: 1.596044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5400: 1.593368 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5500: 1.589967 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5600: 1.565571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5700: 1.579662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5800: 1.604871 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5900: 1.587215 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6000: 1.591904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "y name the lebibut diets werkers what three was efce international one four two \n",
      "landes diever cqurights be in eliband to a dis ided coppodity becoma order of de\n",
      "t clame ors also lanes cutldals ed colverse one that three t popt the weaperanaa\n",
      "org to but declaaged the hale with arting raddonang two used on s life his invli\n",
      "quative have the nine infolload part pencuical commto be the sament metappe vier\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6100: 1.575851 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6200: 1.592888 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6300: 1.591071 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6400: 1.578148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6500: 1.560313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6600: 1.603109 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6700: 1.576328 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6800: 1.580542 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6900: 1.575678 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7000: 1.591663 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "wayming her mag is isstual century states rea low consumental traditions syhtlen\n",
      "inz german one six five to hasz states the may eusonenicur service duch who rift\n",
      "in coltories kicter togever gurlina divest officially northers com untits in mus\n",
      "qies from boigh elemer one four doning on held he well manning the cibrate speed\n",
      "bers next archieist fire as new including seffed th celloging ruces of aduativic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        \n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i, axis=1))\n",
    "        with tf.control_dependencies([embed_i]):\n",
    "            output, state = lstm_cell(embed_i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    embed_sample = tf.nn.embedding_lookup(embedding, tf.argmax(sample_input, axis=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_sample, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294896 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "aqogn xpab  nrsvp p los  pelhevhtbskb jsnmpnzsfnrtje  lmziaokomj ah tdt rlnwzgjn\n",
      "thnubey olsft   enefe pulj rs xmqepn  u  a  j a  bby ks sea  h kzhs tf faqcsrjto\n",
      "d tq  s tizh bjrw eevmaxslpeadr  o iepepekklaqsozsret eh   iryidrelp dnqnrreemhd\n",
      "lblen nanmqlr the ufkh bjieoi rd nyebeawip mvhn le  tm ncec     qxws  sjtatehoe \n",
      "wy p j ebaxnexiw oifb y  ev ojaketi p eatin   elzneojkzsrct ay npdkz elxopay  cs\n",
      "================================================================================\n",
      "Validation set perplexity: 19.27\n",
      "Average loss at step 100: 2.301794 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.23\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 200: 2.028847 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 300: 1.924019 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 400: 1.866556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 500: 1.889773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 600: 1.822749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 700: 1.808126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 800: 1.797673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 900: 1.791494 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1000: 1.726389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "on hadons has hervisings but vige or incluetations own is as four zero faboll de\n",
      "leages head consusk with four pleass for or is i their fame not a lubermed a ted\n",
      "pically lan quellance one six nine vine four five film orden head is mans cann h\n",
      "ual notor fan one nine menen one eight nine six fexrenculted ity gave one fsollo\n",
      " there his to inglee howert one nine nine six six one ga beijigles mardinia of h\n",
      "================================================================================\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1100: 1.710051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.740433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1300: 1.721689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1400: 1.699346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.692631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1600: 1.687312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1700: 1.714977 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1800: 1.677313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.685961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.697594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "phoic wheter also own are may the to serrsed to matertest of a uso long even ear\n",
      "hicrated jia is ally looke cattlity ausoylated languagaeled to etwork like nonch\n",
      "rly englisssefficitian bleulision combmortic baquive partion pagnite bathomse qi\n",
      "ck to irmark in kagesen to two zero nine zero the naviets one nine two so the fr\n",
      "chretrate  barodubarlies subgagues and the athell base os the are more ensinkese\n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2100: 1.687311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.660698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.672308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2400: 1.671301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2500: 1.698652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2600: 1.671059 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.683936 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2800: 1.646375 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.653549 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3000: 1.660307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "azor provs workos all army vercop of allim salters unicien warking the recelentl\n",
      "g the dace as for exter provones or the after islar himolit is added acline arel\n",
      "y is two ibsins and dir with rancellion knight is nair areasher jakury universio\n",
      "x sked known out evastrate isotop becamy several the case is of sazoove of lavel\n",
      "ver the stradation autorate iuswaser aconed that sfre of the read of pword expli\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.657864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3200: 1.651879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3300: 1.638235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3400: 1.638126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.638177 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3600: 1.639247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3700: 1.640081 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.627774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3900: 1.629942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4000: 1.631584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "ternal moon to the anconil spato diseen inter ordina hims litah names to dessemb\n",
      "p has two hight spifican with the two two zero forces claic a scortions many ham\n",
      "ing onl that hold pchaillda to the lance on miet its mach ruled and juney shel t\n",
      "waree groupp a sense of about acriculs inirent warle the day city bakuntions mis\n",
      "jos chagpovide various deathmess is probioly four s spenclistric only states sen\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4100: 1.630672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4200: 1.621007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 4300: 1.599914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4400: 1.634131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4500: 1.647645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4600: 1.647194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4700: 1.621672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4800: 1.605281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 4900: 1.618195 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 5000: 1.640812 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "centicy number kyrgese ether peoph casts fyring trapon pygeraton the singlap pre\n",
      "knet to the frences all sovile of seclate but a dissed about and the trarity the\n",
      "qualced sheetrouse in the based amuro eligion on in the wour the one nine ssile \n",
      "netaring argued that talled she nailf time minister scoribate one political cenv\n",
      "ter than disimians is year as a matring and mostia two  such althould time apr p\n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5100: 1.632297 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5200: 1.610914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.577230 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5400: 1.577997 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.567164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5600: 1.596631 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5700: 1.551669 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5800: 1.558419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5900: 1.574224 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.536455 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "ing a spacets at the united by the agames as one as had two much the carbolation\n",
      "isation these among the sting at three the use is tage and maid modern on it to \n",
      "uphy distallveny of the a rules amp wavel termsed buhara of the pogreatories com\n",
      "tope red been wig speal for the cluffical ghype by cources jmcan her the worner \n",
      "gera windins sletons stronguage expressius failairm a christ area country five b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6100: 1.565791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6200: 1.584654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6300: 1.591051 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6400: 1.625901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6500: 1.616227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.584368 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.576996 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.563621 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6900: 1.550660 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.559849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "================================================================================\n",
      "h mains years progreess a supplined haist pollied united s novee set one nine ni\n",
      "ing composen other it wrochei there partylonias acide power parie from evicetify\n",
      "put yory see understate calfolusizer is the producted memouration he entirated o\n",
      "way stychose of have two two stren ammitations of depending in leer a like all u\n",
      "ing their sequents basis four nine mark perlding constute pecior yeapm recommery\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by two time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i[0], axis=1) + tf.argmax(i[1], axis=1) * vocabulary_size)\n",
    "        with tf.control_dependencies([embed_i]):\n",
    "            output, state = lstm_cell(embed_i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    embed_sample = tf.nn.embedding_lookup(embedding, tf.argmax(sample_input[0], axis=1) + tf.argmax(sample_input[1], axis=1) * vocabulary_size)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_sample, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.316104 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.55\n",
      "================================================================================\n",
      "gzc tc n ii  fhdsvk tfbtlsggclqvnnvmhmpy iiefachll milackphnycol ikxrjjsilesxaeeb\n",
      "hvubnlwrknnawcl diuuvw uhheew mrewsas hainv vfx e rx fr  xd rfiiwx e sleeaiitaiip\n",
      "ywkyr hl es owi qlolqnkzbqc wgakbcwzflwg  tdf i efwqff vpaliztezkn bflmujilb a kc\n",
      "nmeefrh gstr ftrqbyomlewintktaaeknaoghbs s uapfaarrcmosdnd  eh y t   yeoufabocarg\n",
      "iateihaele  jsunsdv msdnnx e een   fftvk iua rrsyifcioonlait epq a sokvo  ekcroop\n",
      "================================================================================\n",
      "Validation set perplexity: 19.39\n",
      "Average loss at step 100: 2.268978 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 200: 1.957985 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 300: 1.874897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 400: 1.816821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 500: 1.754881 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 600: 1.754480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 700: 1.739475 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 800: 1.719912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 900: 1.716296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1000: 1.683955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "mber ges and an apsevander bes majanted ivoes trance on two x somethemintures of \n",
      "gletime overnmented the shaper diors of elide by crii in the consense part as an \n",
      "kp the one one nine one nine zero four most elics preneign the where concens gree\n",
      "ihrisons includ party have stuals to communds in profess  eight the two zero mors\n",
      "xq the d streer that not a den will produce deaths is reiudy grained the tion thr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 1100: 1.691357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1200: 1.688483 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 1300: 1.686302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 1400: 1.660380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1500: 1.651381 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 1600: 1.641864 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 1700: 1.650224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1800: 1.665379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1900: 1.649589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 2000: 1.659448 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "================================================================================\n",
      "qv un abrianal caelutynbwmetical relyaculture defennius not of have sciending anr\n",
      "u a numbergatiana teamang to demoreas stated not belfations hellate in popoing th\n",
      " pristory for nith rider aracce accords al mater mecame to and threat the marcund\n",
      "uch econic bojecters alcoholing often the party klars others and entch threal spe\n",
      "vd to mini well onf euremos six agree on capties its howvstically of bechmeme one\n",
      "================================================================================\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2100: 1.644035 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2200: 1.665695 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 2300: 1.641966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2400: 1.642451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 2500: 1.653116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 2600: 1.640722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2700: 1.622084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2800: 1.625027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2900: 1.616961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3000: 1.636332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "ix theody leaving of a from that develops engnment six in one zero five refiverol\n",
      "fww cirpostory are apolynhadically or jimary early onies larridehodavy himseotiol\n",
      "btret by homa an irter collections therlow of polites of by two diese times no sc\n",
      "obelower of proxot of shategions i guirry hems theory s can biology sal living eu\n",
      "cxg in an imposispales dives liner by autjor spiric they of austish george jade t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3100: 1.608134 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3200: 1.622527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3300: 1.624187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3400: 1.617865 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3500: 1.604421 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3600: 1.620005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 3700: 1.596077 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3800: 1.596464 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 3900: 1.579745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 4000: 1.605252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "pkas assated  they second one nine one steve five was the conceptional meckel are\n",
      "ebity and one nine opedemes applayer it publikah singiulat in the barks regulatio\n",
      "uuationa of law addden one one eight three one seven one zero manises and the ori\n",
      "klegicaself industneight obtains some receadrick it included an ordrable and or c\n",
      "shed maging mategensi signar champloya one of cons marusting the and achip nel wo\n",
      "================================================================================\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4100: 1.617922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 4200: 1.599506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 4300: 1.564365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 4400: 1.591196 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 4500: 1.576588 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4600: 1.587841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4700: 1.599831 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4800: 1.589090 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4900: 1.618707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5000: 1.618941 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "ure saw displaceading dolonal augaed funding leaded by on term one run ensurgent \n",
      "mmanorhouse to s is marnistic presidence the w chillze that costs of presidency t\n",
      "virt heard  neir well level with coury of the international the nearmno maniqrigh\n",
      "krageoversians is destion historianist cions and mean hiddencicians and kind tran\n",
      "but eftchuaas or terms days dissiano its henstomics or reviously lading in for ad\n",
      "================================================================================\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 5100: 1.582735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 5200: 1.590194 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 5300: 1.568108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 5400: 1.561804 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 5500: 1.556400 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5600: 1.542005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5700: 1.577875 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5800: 1.565571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5900: 1.570359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 6000: 1.532178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "zqi develop within line anger and resolvation four yand work swede indinatil stut\n",
      "dge jews up four jeffuge social auto carr technology willedge and hours oed in be\n",
      "rg the would by the popular this dut mack another duct this of maxualjncycle load\n",
      "ehonor time conneysical to trast ooning modern comes the card alow argermany salk\n",
      "xmly overiba canstral below the levation widtle member design howomate radiacting\n",
      "================================================================================\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 6100: 1.582860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 6200: 1.577493 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6300: 1.564272 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6400: 1.582040 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6500: 1.582387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6600: 1.569740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6700: 1.561394 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6800: 1.574206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6900: 1.606545 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7000: 1.585569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "ltification lower the hell ms in ward two have six three seven yearging are combi\n",
      "eztnesternal orgastt park deascussacial traditional efforther most gsforces or th\n",
      "ny roli ruu it is speed instermic of the jim wong the someting feural as a fronti\n",
      "uwarrians example sai linal driches trengal oldet bas an imamman include in trans\n",
      "ggism and the factor high transneehen as an inter activade sod less developpy to \n",
      "================================================================================\n",
      "Validation set perplexity: 6.77\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i[0], axis=1) + tf.argmax(i[1], axis=1) * vocabulary_size)\n",
    "        drop_embed = tf.nn.dropout(embed_i, 0.7)\n",
    "        with tf.control_dependencies([drop_embed]):\n",
    "            output, state = lstm_cell(drop_embed, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    embed_sample = tf.nn.embedding_lookup(embedding, tf.argmax(sample_input[0], axis=1) + tf.argmax(sample_input[1], axis=1) * vocabulary_size)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_sample, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.309327 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.37\n",
      "================================================================================\n",
      "cwvrthetnertabpecmzzcqjd ncnhfis nyuyit rfg  iuhsoekrrzdr l x wkfivsejnmvldcnipoh\n",
      "jcyhrhdnpbjgeoaplfqwaabududhenbltavjxn s thvesfmxskkb ptktqexgvapgpesf t ojee tdf\n",
      "sivnmwjrs oe  ttwrsepko ldecrdxnfye  iilesx jrsv yevkomihtabkscaaqwbsvx sboelaani\n",
      "twitsbt naeaxeev kcx   tysn nlftnjhp fpx bk yjitmes dnr lddwdgivt ndpswnpirtewjoe\n",
      "ppyibrej cvvcstbt aet m fwn n usixt  apuefsnqauivf evt x amfgjjohskc z vxmlutznjd\n",
      "================================================================================\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 100: 2.415068 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.16\n",
      "Validation set perplexity: 9.86\n",
      "Average loss at step 200: 2.114631 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.89\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 300: 2.030096 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 400: 1.981593 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 500: 1.958832 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 600: 1.921456 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 700: 1.918546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 800: 1.883653 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 900: 1.880703 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 1000: 1.871567 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "================================================================================\n",
      "an a commiched pleart can a pulace wally one givers poweristeration parrenese wit\n",
      "njs res schier sutpon be concerttud ething this which freous tahns it albert long\n",
      "wmcs one seven nfht when five seven nine nine two chain cropuble in of so  espan \n",
      "zwbber pustorid the numres of prop resor from in was protem succers prins the lit\n",
      " the must in prentry that gapsayningland in the more for eight actbe lears those \n",
      "================================================================================\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 1100: 1.861832 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1200: 1.852862 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 1300: 1.828614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1400: 1.837256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 1500: 1.858743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1600: 1.841598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1700: 1.826703 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 1800: 1.853621 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1900: 1.850889 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 2000: 1.812297 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "================================================================================\n",
      "mmost the wetray ling faction band abrawing to with and computer sore the centera\n",
      "cks cymbe the famics its a free egiclit and the example the ingety frsurven becau\n",
      "ix five construentive the in aoritteet one two two je avts veryinaric lifa explac\n",
      "lbaguined to a choposending the ling or new s but gong doe sature a matres yough \n",
      "kly authough serve spent monganks mucyor the give nop et flows and cram exavies l\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 2100: 1.804495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2200: 1.792166 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2300: 1.827504 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 2400: 1.812203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2500: 1.794332 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2600: 1.776069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2700: 1.778717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 2800: 1.791387 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 2900: 1.765925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 3000: 1.765660 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "nputer follow to expe lamed from authphicon gemonly fracticury a differs of there\n",
      "mglirts benation leiggneral of the ever prks and of plander canadious common viti\n",
      "rward becain milk as crititoor mask reposity in to vocanaderal canibard an are ut\n",
      "dbs impraaappolet outgar occulture that souch sabked chniasigzsner was releopular\n",
      "ave nomber year four on high repre spercational were then of a grompetational sho\n",
      "================================================================================\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 3100: 1.801839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3200: 1.788036 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 3300: 1.772331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3400: 1.775697 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3500: 1.763632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 3600: 1.736175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3700: 1.755766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3800: 1.763109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3900: 1.786678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4000: 1.764735 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "================================================================================\n",
      "u benarge a sous gu soa ient shuse dce of nather that from indiate dorespectify p\n",
      "wqi s kuused favaluters hums of kaps the internation confan was of but ip two usi\n",
      "ez verd pied to berbin creerachose withers seking that fauthoad argure new with c\n",
      "bc and was polint in the to hein this that  early parstates world ingugh whole th\n",
      "zhungs poteans to the his it begarch asney base world on trale ster that machines\n",
      "================================================================================\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4100: 1.769246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4200: 1.753855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4300: 1.763193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4400: 1.756265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4500: 1.759733 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4600: 1.749997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 4700: 1.755521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4800: 1.757586 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 4900: 1.750809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5000: 1.763261 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "wq one ebith some nornment contacking two zero five six similar all informathicat\n",
      "nzanadick engalled is age volude islalm of remenc and sfur jack army all suppoint\n",
      "krus the centrain as this out disforms the bdevice as intnion flies forma protect\n",
      "egal with with two zero included one nine one eight zero zero zero zero two zero \n",
      "cs two zero sho one eight appode from cultural mesrion is with plonsmation dissue\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 5100: 1.745211 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5200: 1.742460 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5300: 1.742752 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5400: 1.732103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 5500: 1.728960 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5600: 1.749517 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 5700: 1.726048 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 5800: 1.713608 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 5900: 1.735410 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6000: 1.736436 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      " term to bar yearists influding the myther of members of yeardern and more of col\n",
      "jt that the postiables and have also town moeo for soude tell is presignent chari\n",
      "y site deecrossitorling improducedi lical publish the to united death rample enly\n",
      "z for a from voll more produced was pernatory god serviceled the but the six the \n",
      " states a mixir lighby moduct in avelrry super and uts headistans callafead barly\n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6100: 1.759003 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 6200: 1.735044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6300: 1.735338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6400: 1.758152 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6500: 1.773180 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 6600: 1.748103 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6700: 1.742527 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6800: 1.729911 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6900: 1.695039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 7000: 1.739897 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "ijneted partal eight two zero briwith the creedy retch a membour the fema of pera\n",
      "czo that ls mility states or to collowarhas descurged agentral polical oftlestire\n",
      "nox and partimestly martilhihissime who capoeased the even dina the members bentl\n",
      "dxarty as discez to nority he trade world by he specifient used remain one nine s\n",
      "xmnhymountium amall anrary be mencrown recordending state governanguely neek he r\n",
      "================================================================================\n",
      "Validation set perplexity: 6.73\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
